{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"Fraud-Dection.ipynb","provenance":[],"collapsed_sections":["lDXqWjIKfRdz","Wpce1N84fRdz","CiaZ1WtLfRdz"],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"y6PP7EQqfRdw"},"source":["It's Sunday morning, it's quiet and you wake up with a big smile on your face. Today is going to be a great day! Except, your phone rings, rather \"internationally\". You pick it up slowly and hear something really bizarre - \"Bonjour, je suis Michele. Oops, sorry. I am Michele, your personal bank agent.\". What could possibly be so urgent for someone from Switzerland to call you at this hour? \"Did you authorize a transaction for $3,358.65 for 100 copies of Diablo 3?\" Immediately, you start thinking of ways to explain why you did that to your loved one. \"No, I didn't !?\". Michele's answer is quick and to the point - \"Thank you, we're on it\". Whew, that was close! But how did Michele knew that this transaction was suspicious? After all, you did order 10 new smartphones from that same bank account, last week - Michele didn't call then.\n","\n","![](https://cdn.tutsplus.com/net/uploads/legacy/2061_stripe/1.png)\n","\n","Annual global fraud losses reached $21.8 billion in 2015, according to [Nilson Report](https://www.nilsonreport.com/upload/content_promo/The_Nilson_Report_10-17-2016.pdf). \n","\n","Probably you feel very lucky if you are a fraud. About every 12 cents per $100 were stolen in the US during the same year. Our friend Michele might have a serious problem to solve here.\n","\n","In this part of the series, we will train an Autoencoder Neural Network (implemented in Keras) in unsupervised (or semi-supervised) fashion for Anomaly Detection in credit card transaction data. The trained model will be evaluated on pre-labeled and anonymized dataset.\n","\n","# Setup\n","\n","We will be using TensorFlow 1.2 and Keras 2.0.4. Let's begin:"]},{"cell_type":"code","metadata":{"id":"wML3KKKdfRdw"},"source":["import pandas as pd\n","import numpy as np\n","import pickle\n","import matplotlib.pyplot as plt\n","from scipy import stats\n","import tensorflow as tf\n","import seaborn as sns\n","from pylab import rcParams\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import Input, Dense\n","from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n","from tensorflow.keras import regularizers\n","\n","%matplotlib inline\n","\n","sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n","\n","rcParams['figure.figsize'] = 14, 8\n","\n","RANDOM_SEED = 42\n","LABELS = [\"Normal\", \"Fraud\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ydCwhgYcfRdx"},"source":["# Loading the data\n","\n","The dataset we're going to use can be downloaded from [Kaggle](https://www.kaggle.com/dalpozz/creditcardfraud). It contains data about credit card transactions that occurred during a period of two days, with 492 frauds out of 284,807 transactions.\n","\n","All variables in the dataset are numerical. The data has been transformed using PCA transformation(s) due to privacy reasons. The two features that haven't been changed are Time and Amount. Time contains the seconds elapsed between each transaction and the first transaction in the dataset."]},{"cell_type":"code","metadata":{"id":"7p6_STsgxZhv"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"COZxE90IfRdx"},"source":["![ ! -f creditcardfraud.zip ] && wget https://raw.githubusercontent.com/Finfra/AICentroPreLab/master/data/creditcardfraud.zip\n","![ ! -f creditcard.csv ]&& (unzip creditcardfraud.zip)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HM05wF24fRdx"},"source":["df = pd.read_csv(\"creditcard.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q5jf1tk9fRdx"},"source":["# Exploration"]},{"cell_type":"code","metadata":{"id":"Qb9iiD7mfRdx"},"source":["df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tV7CWgmdfRdy"},"source":["31 columns, 2 of which are Time and Amount. The rest are output from the PCA transformation. Let's check for missing values:"]},{"cell_type":"code","metadata":{"id":"_0ch9F9VfRdy"},"source":["df.isnull().values.any()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Up0Q24lKfRdy"},"source":["count_classes = pd.value_counts(df['Class'], sort = True)\n","count_classes.plot(kind = 'bar', rot=0)\n","plt.title(\"Transaction class distribution\")\n","plt.xticks(range(2), LABELS)\n","plt.xlabel(\"Class\")\n","plt.ylabel(\"Frequency\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cOPmAvhhfRdy"},"source":["We have a highly imbalanced dataset on our hands. Normal transactions overwhelm the fraudulent ones by a large margin. Let's look at the two types of transactions: "]},{"cell_type":"code","metadata":{"id":"pIwdIu1dfRdy"},"source":["frauds = df[df.Class == 1]\n","normal = df[df.Class == 0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nd4Cdb-8fRdy"},"source":["frauds.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pLgV6PqqfRdy"},"source":["normal.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i-gE2SeLfRdy"},"source":["How different are the amount of money used in different transaction classes?"]},{"cell_type":"code","metadata":{"id":"CUuT1KpYfRdy"},"source":["frauds.Amount.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"9BEInz1FfRdy"},"source":["normal.Amount.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FaiKvC_cfRdy"},"source":["Let's have a more graphical representation:"]},{"cell_type":"code","metadata":{"id":"FoUqsSqzfRdy"},"source":["f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n","f.suptitle('Amount per transaction by class')\n","\n","bins = 50\n","\n","ax1.hist(frauds.Amount, bins = bins)\n","ax1.set_title('Fraud')\n","\n","ax2.hist(normal.Amount, bins = bins)\n","ax2.set_title('Normal')\n","\n","plt.xlabel('Amount ($)')\n","plt.ylabel('Number of Transactions')\n","plt.xlim((0, 20000))\n","plt.yscale('log')\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y2LhuzpufRdy"},"source":["Do fraudulent transactions occur more often during certain time?"]},{"cell_type":"code","metadata":{"id":"qpODH6FGfRdy"},"source":["f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n","f.suptitle('Time of transaction vs Amount by class')\n","\n","ax1.scatter(frauds.Time, frauds.Amount)\n","ax1.set_title('Fraud')\n","\n","ax2.scatter(normal.Time, normal.Amount)\n","ax2.set_title('Normal')\n","\n","plt.xlabel('Time (in Seconds)')\n","plt.ylabel('Amount')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YxXjtyyefRdy"},"source":["Doesn't seem like the time of transaction really matters.\n","\n","# Autoencoders\n","\n","Autoencoders can seem quite bizarre at first. The job of those models is to predict the input, given that same input. Puzzling? Definitely was for me, the first time I heard it.\n","\n","More specifically, letâ€™s take a look at Autoencoder Neural Networks. This autoencoder tries to learn to approximate the following identity function:\n","\n","$$\\textstyle f_{W,b}(x) \\approx x$$\n","\n","While trying to do just that might sound trivial at first, it is important to note that we want to learn a compressed representation of the data, thus find structure. This can be done by limiting the number of hidden units in the model. Those kind of autoencoders are called *undercomplete*.\n","\n","Here's a visual representation of what an Autoencoder might learn:\n","\n","![](img/mushroom_encoder.png)\n","\n","## Reconstruction error\n","\n","We optimize the parameters of our Autoencoder model in such way that a special kind of error - reconstruction error is minimized. In practice, the traditional squared error is often used:\n","\n","$$\\textstyle L(x,x') = ||\\, x - x'||^2$$\n","\n","If you want to learn more about Autoencoders I highly recommend the following videos by Hugo Larochelle:\n","\n","<iframe width=\"100%\" height=\"480\" src=\"https://www.youtube.com/embed/FzS3tMl4Nsc\" frameborder=\"0\" allowfullscreen></iframe>"]},{"cell_type":"markdown","metadata":{"id":"M-tHViiqfRdy"},"source":["# Preparing the data\n","\n","First, let's drop the Time column (not going to use it) and use the scikit's StandardScaler on the Amount. The scaler removes the mean and scales the values to unit variance:"]},{"cell_type":"code","metadata":{"id":"lFMNX22YfRdy"},"source":["from sklearn.preprocessing import StandardScaler\n","\n","data = df.drop(['Time'], axis=1)\n","\n","data['Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8P95qeGGfRdy"},"source":["Training our Autoencoder is gonna be a bit different from what we are used to. Let's say you have a dataset containing a lot of non fraudulent transactions at hand. You want to detect any anomaly on new transactions. We will create this situation by training our model on the normal transactions, only. Reserving the correct class on the test set will give us a way to evaluate the performance of our model. We will reserve 20% of our data for testing:"]},{"cell_type":"code","metadata":{"id":"FQ8Kzv8JfRdy"},"source":["X_train, X_test = train_test_split(data, test_size=0.2, random_state=RANDOM_SEED)\n","X_train = X_train[X_train.Class == 0]\n","X_train = X_train.drop(['Class'], axis=1)\n","\n","y_test = X_test['Class']\n","X_test = X_test.drop(['Class'], axis=1)\n","\n","X_train = X_train.values\n","X_test = X_test.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_1pvsYYMfRdy"},"source":["X_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"60tFDc_bfRdy"},"source":["# Building the model\n","\n","Our Autoencoder uses 4 fully connected layers with 14, 7, 7 and 29 neurons respectively.  The first two layers are used for our encoder, the last two go for the decoder. Additionally, L1 regularization will be used during training:"]},{"cell_type":"code","metadata":{"id":"CKtlFQmFfRdz"},"source":["input_dim = X_train.shape[1]\n","encoding_dim = 14"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lQ7tBuYqfRdz"},"source":["input_layer = Input(shape=(input_dim, ))\n","\n","encoder = Dense(encoding_dim, activation=\"tanh\", \n","                activity_regularizer=regularizers.l1(10e-5))(input_layer)\n","encoder = Dense(int(encoding_dim / 2), activation=\"relu\")(encoder)\n","\n","decoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)\n","decoder = Dense(input_dim, activation='relu')(decoder)\n","\n","autoencoder = Model(inputs=input_layer, outputs=decoder)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VMWD5qKZfRdz"},"source":["Let's train our model for 100 epochs with a batch size of 32 samples and save the best performing model to a file. The ModelCheckpoint provided by Keras is really handy for such tasks. Additionally,  the training progress will be exported in a format that TensorBoard understands."]},{"cell_type":"code","metadata":{"id":"U-fCJup7fRdz"},"source":["nb_epoch = 100\n","batch_size = 32\n","\n","autoencoder.compile(optimizer='adam', \n","                    loss='mean_squared_error', \n","                    metrics=['accuracy'])\n","\n","checkpointer = ModelCheckpoint(filepath=\"model.h5\",\n","                               verbose=0,\n","                               save_best_only=True)\n","tensorboard = TensorBoard(log_dir='./logs',\n","                          histogram_freq=0,\n","                          write_graph=True,\n","                          write_images=True)\n","\n","history = autoencoder.fit(X_train, X_train,\n","                    epochs=nb_epoch,\n","                    batch_size=batch_size,\n","                    shuffle=True,\n","                    validation_data=(X_test, X_test),\n","                    verbose=1,\n","                    callbacks=[checkpointer, tensorboard]).history"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"45vy0vsNfRdz"},"source":["autoencoder = load_model('model.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l7nJ1jrufRdz"},"source":["# Evaluation"]},{"cell_type":"code","metadata":{"id":"39ah20ogfRdz"},"source":["plt.plot(history['loss'])\n","plt.plot(history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper right');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pVLzZiXJfRdz"},"source":["The reconstruction error on our training and test data seems to converge nicely. Is it low enough? Let's have a closer look at the error distribution:"]},{"cell_type":"code","metadata":{"id":"JLSPkLJzfRdz"},"source":["predictions = autoencoder.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ThFoGtNfRdz"},"source":["mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n","error_df = pd.DataFrame({'reconstruction_error': mse,\n","                        'true_class': y_test})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T3LOV34FfRdz"},"source":["error_df.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lDXqWjIKfRdz"},"source":["## Reconstruction error without fraud"]},{"cell_type":"code","metadata":{"id":"mfCvZqG2fRdz"},"source":["fig = plt.figure()\n","ax = fig.add_subplot(111)\n","normal_error_df = error_df[(error_df['true_class']== 0) & (error_df['reconstruction_error'] < 10)]\n","_ = ax.hist(normal_error_df.reconstruction_error.values, bins=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wpce1N84fRdz"},"source":["## Reconstruction error with fraud"]},{"cell_type":"code","metadata":{"id":"hor_BYFnfRdz"},"source":["fig = plt.figure()\n","ax = fig.add_subplot(111)\n","fraud_error_df = error_df[error_df['true_class'] == 1]\n","_ = ax.hist(fraud_error_df.reconstruction_error.values, bins=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QNbp-p-bfRdz"},"source":["from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n","                             roc_curve, recall_score, classification_report, f1_score,\n","                             precision_recall_fscore_support)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SmXyCJamfRdz"},"source":["ROC curves are very useful tool for understanding the performance of binary classifiers. However, our case is a bit out of the ordinary. We have a very imbalanced dataset. Nonetheless, let's have a look at our ROC curve:"]},{"cell_type":"code","metadata":{"id":"Am5bpIzzfRdz"},"source":["fpr, tpr, thresholds = roc_curve(error_df.true_class, error_df.reconstruction_error)\n","roc_auc = auc(fpr, tpr)\n","\n","plt.title('Receiver Operating Characteristic')\n","plt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n","plt.legend(loc='lower right')\n","plt.plot([0,1],[0,1],'r--')\n","plt.xlim([-0.001, 1])\n","plt.ylim([0, 1.001])\n","plt.ylabel('True Positive Rate')\n","plt.xlabel('False Positive Rate')\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CiaZ1WtLfRdz"},"source":["The ROC curve plots the true positive rate versus the false positive rate, over different threshold values. Basically, we want the blue line to be as close as possible to the upper left corner. While our results look pretty good, we have to keep in mind of the nature of our dataset. ROC doesn't look very useful for us. Onward...\n","\n","## Precision vs Recall\n","\n","<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png\" />\n","\n","Precision and recall are defined as follows:\n","\n","$$\\text{Precision} = \\frac{\\text{true positives}}{\\text{true positives} + \\text{false positives}}$$\n","\n","$$\\text{Recall} = \\frac{\\text{true positives}}{\\text{true positives} + \\text{false negatives}}$$\n","\n","Let's take an example from Information Retrieval in order to better understand what precision and recall are. Precision measures the relevancy of obtained results. Recall, on the other hand, measures how many relevant results are returned. Both values can take values between 0 and 1. You would love to have a system with both values being equal to 1.\n","\n","Let's return to our example from Information Retrieval. High recall but low precision means many results, most of which has low or no relevancy. When precision is high but recall is low we have the opposite - few returned results with very high relevancy. Ideally, you would want high precision and high recall - many results with that are highly relevant."]},{"cell_type":"code","metadata":{"id":"tgq3tOFtfRdz"},"source":["precision, recall, th = precision_recall_curve(error_df.true_class, error_df.reconstruction_error)\n","plt.plot(recall, precision, 'b', label='Precision-Recall curve')\n","plt.title('Recall vs Precision')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4_f3U1OxfRdz"},"source":["A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall)."]},{"cell_type":"code","metadata":{"id":"UFCTdL13fRdz"},"source":["plt.plot(th, precision[1:], 'b', label='Threshold-Precision curve')\n","plt.title('Precision for different threshold values')\n","plt.xlabel('Threshold')\n","plt.ylabel('Precision')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sb_M7QV2fRdz"},"source":["You can see that as the reconstruction error increases our precision rises as well. Let's have a look at the recall:"]},{"cell_type":"code","metadata":{"id":"0RZrXiW7fRdz"},"source":["plt.plot(th, recall[1:], 'b', label='Threshold-Recall curve')\n","plt.title('Recall for different threshold values')\n","plt.xlabel('Reconstruction error')\n","plt.ylabel('Recall')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kg6PGbf4fRdz"},"source":["Here, we have the exact opposite situation. As the reconstruction error increases the recall decreases."]},{"cell_type":"markdown","metadata":{"id":"spuBgSMifRdz"},"source":["# Prediction\n","\n","Our model is a bit different this time. It doesn't know how to predict new values. But we don't need that. In order to predict whether or not a new/unseen transaction is normal or fraudulent, we'll calculate the reconstruction error from the transaction data itself. If the error is larger than a predefined threshold, we'll mark it as a fraud (since our model should have a low error on normal transactions). Let's pick that value:"]},{"cell_type":"code","metadata":{"id":"4oPsHq5ofRdz"},"source":["threshold = 25"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uhyG0OGdfRdz"},"source":["And see how well we're dividing the two types of transactions:"]},{"cell_type":"code","metadata":{"id":"LrK2hcAKfRdz"},"source":["groups = error_df.groupby('true_class')\n","fig, ax = plt.subplots()\n","\n","for name, group in groups:\n","    ax.plot(group.index, group.reconstruction_error, marker='o', ms=3.5, linestyle='',\n","            label= \"Fraud\" if name == 1 else \"Normal\")\n","ax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\n","ax.legend()\n","plt.title(\"Reconstruction error for different classes\")\n","plt.ylabel(\"Reconstruction error\")\n","plt.xlabel(\"Data point index\")\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m9bU6djDfRdz"},"source":["I know, that chart might be a bit deceiving. Let's have a look at the confusion matrix:"]},{"cell_type":"code","metadata":{"id":"-HZF82BzfRd0"},"source":["y_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n","conf_matrix = confusion_matrix(error_df.true_class, y_pred)\n","\n","plt.figure(figsize=(12, 12))\n","sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n","plt.title(\"Confusion matrix\")\n","plt.ylabel('True class')\n","plt.xlabel('Predicted class')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iaCxnRybfRd0"},"source":["Our model seems to catch a lot of the fraudulent cases. Of course, there is a catch (see what I did there?). The number of normal transactions classified as frauds is really high. Is this really a problem? Probably it is. You might want to increase or decrease the value of the threshold, depending on the problem. That one is up to you.\n","\n","# Conclusion\n","\n","We've created a very simple Deep Autoencoder in Keras that can reconstruct what non fraudulent transactions looks like. Initially, I was a bit skeptical about whether or not this whole thing is gonna work out, bit it kinda did. Think about it, we gave a lot of one-class examples (normal transactions) to a model and it learned (somewhat) how to discriminate whether or not new examples belong to that same class. Isn't that cool? Our dataset was kind of magical, though. We really don't know what the original features look like.\n","\n","Keras gave us very clean and easy to use API to build a non-trivial Deep Autoencoder. You can search for TensorFlow implementations and see for yourself how much boilerplate you need in order to train one. Can you apply a similar model to a different problem?\n","\n","# References\n","\n","- [Building Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)\n","- [Stanford tutorial on Autoencoders](http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/)\n","- [Stacked Autoencoders in TensorFlow](http://cmgreen.io/2016/01/04/tensorflow_deep_autoencoder.html)"]},{"cell_type":"code","metadata":{"id":"gqv5Lv99fRd0"},"source":[""],"execution_count":null,"outputs":[]}]}