{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DCGan","provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/generative/ipynb/dcgan_overriding_train_step.ipynb","timestamp":1606877303975}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"RE4APM3NWbcp"},"source":["# GAN overriding `Model.train_step`\n","\n","**Author:** [fchollet](https://twitter.com/fchollet)<br>\n","**Date created:** 2019/04/29<br>\n","**Last modified:** 2020/04/29<br>\n","**Description:** A simple DCGAN trained using `fit()` by overriding `train_step`."]},{"cell_type":"markdown","metadata":{"id":"Q_Eo7ZVJWbcq"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"DekunSMmWbcq"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hbGIPRiKWbcq"},"source":["## Prepare MNIST data"]},{"cell_type":"code","metadata":{"id":"tDAzpbWuWbcq"},"source":["# We use both the training & test MNIST digits.\n","batch_size = 32\n","(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n","all_digits = np.concatenate([x_train, x_test])\n","all_digits = all_digits.astype(\"float32\") / 255\n","all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n","dataset = tf.data.Dataset.from_tensor_slices(all_digits)\n","dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZYHeI9msWbcq"},"source":["## Create the discriminator\n","\n","It maps 28x28 digits to a binary classification score."]},{"cell_type":"code","metadata":{"id":"0o-FKh2qWbcq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606882756977,"user_tz":-540,"elapsed":3105,"user":{"displayName":"남중구","photoUrl":"","userId":"02914767085172164696"}},"outputId":"1fe991a0-4cb4-4d86-c214-637ec3dd5d20"},"source":["discriminator = keras.Sequential(\n","    [\n","        keras.Input(shape=(28, 28, 1)),\n","        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.GlobalMaxPooling2D(),\n","        layers.Dense(1),\n","    ],\n","    name=\"discriminator\",\n",")\n","\n","discriminator.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"discriminator\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 14, 14, 64)        640       \n","_________________________________________________________________\n","leaky_re_lu (LeakyReLU)      (None, 14, 14, 64)        0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 7, 7, 128)         73856     \n","_________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)    (None, 7, 7, 128)         0         \n","_________________________________________________________________\n","global_max_pooling2d (Global (None, 128)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 1)                 129       \n","=================================================================\n","Total params: 74,625\n","Trainable params: 74,625\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"at4C5JFbWbcq"},"source":["## Create the generator\n","\n","It mirrors the discriminator, replacing `Conv2D` layers with `Conv2DTranspose` layers."]},{"cell_type":"code","metadata":{"id":"t5MxTaxJWbcq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606882756977,"user_tz":-540,"elapsed":3099,"user":{"displayName":"남중구","photoUrl":"","userId":"02914767085172164696"}},"outputId":"03da983f-0383-449b-85a0-e8ef894fa005"},"source":["latent_dim = 128\n","\n","generator = keras.Sequential(\n","    [\n","        keras.Input(shape=(latent_dim,)),\n","        # We want to generate 128 coefficients to reshape into a 7x7x128 map\n","        layers.Dense(7 * 7 * 128),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Reshape((7, 7, 128)),\n","        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n","    ],\n","    name=\"generator\",\n",")\n","\n","generator.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"generator\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_1 (Dense)              (None, 6272)              809088    \n","_________________________________________________________________\n","leaky_re_lu_2 (LeakyReLU)    (None, 6272)              0         \n","_________________________________________________________________\n","reshape (Reshape)            (None, 7, 7, 128)         0         \n","_________________________________________________________________\n","conv2d_transpose (Conv2DTran (None, 14, 14, 128)       262272    \n","_________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)    (None, 14, 14, 128)       0         \n","_________________________________________________________________\n","conv2d_transpose_1 (Conv2DTr (None, 28, 28, 128)       262272    \n","_________________________________________________________________\n","leaky_re_lu_4 (LeakyReLU)    (None, 28, 28, 128)       0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 28, 28, 1)         6273      \n","=================================================================\n","Total params: 1,339,905\n","Trainable params: 1,339,905\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WGgHHBxHWbcq"},"source":["## Override `train_step`"]},{"cell_type":"code","metadata":{"id":"Spscbl7HWbcq"},"source":["\n","class GAN(keras.Model):\n","    def __init__(self, discriminator, generator, latent_dim):\n","        super(GAN, self).__init__()\n","        self.discriminator = discriminator\n","        self.generator = generator\n","        self.latent_dim = latent_dim\n","\n","    def compile(self, d_optimizer, g_optimizer, loss_fn):\n","        super(GAN, self).compile()\n","        self.d_optimizer = d_optimizer\n","        self.g_optimizer = g_optimizer\n","        self.loss_fn = loss_fn\n","\n","    def train_step(self, real_images):\n","        if isinstance(real_images, tuple):\n","            real_images = real_images[0]\n","        # Sample random points in the latent space\n","        batch_size = tf.shape(real_images)[0]\n","        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n","\n","        # Decode them to fake images\n","        generated_images = self.generator(random_latent_vectors)\n","\n","        # Combine them with real images\n","        combined_images = tf.concat([generated_images, real_images], axis=0)\n","\n","        # Assemble labels discriminating real from fake images\n","        labels = tf.concat(\n","            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n","        )\n","        # Add random noise to the labels - important trick!\n","        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n","\n","        # Train the discriminator\n","        with tf.GradientTape() as tape:\n","            predictions = self.discriminator(combined_images)\n","            d_loss = self.loss_fn(labels, predictions)\n","        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n","        self.d_optimizer.apply_gradients(\n","            zip(grads, self.discriminator.trainable_weights)\n","        )\n","\n","        # Sample random points in the latent space\n","        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n","\n","        # Assemble labels that say \"all real images\"\n","        misleading_labels = tf.zeros((batch_size, 1))\n","\n","        # Train the generator (note that we should *not* update the weights\n","        # of the discriminator)!\n","        with tf.GradientTape() as tape:\n","            predictions = self.discriminator(self.generator(random_latent_vectors))\n","            g_loss = self.loss_fn(misleading_labels, predictions)\n","        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n","        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n","        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0j_JCQoQWbcq"},"source":["## Create a callback that periodically saves generated images"]},{"cell_type":"code","metadata":{"id":"ZPlNMZvrWbcq"},"source":["\n","class GANMonitor(keras.callbacks.Callback):\n","    def __init__(self, num_img=3, latent_dim=128):\n","        self.num_img = num_img\n","        self.latent_dim = latent_dim\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n","        generated_images = self.model.generator(random_latent_vectors)\n","        generated_images *= 255\n","        generated_images.numpy()\n","        for i in range(self.num_img):\n","            img = keras.preprocessing.image.array_to_img(generated_images[i])\n","            img.save(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uUwTszFSWbcq"},"source":["## Train the end-to-end model"]},{"cell_type":"code","metadata":{"id":"qdmd_SYLWbcq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d45ce585-c4ce-4c3e-df50-acf3bb3e4bf7"},"source":["epochs = 100\n","\n","gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n","gan.compile(\n","    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n","    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n","    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",")\n","\n","gan.fit(\n","    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=3, latent_dim=latent_dim)]\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","2188/2188 [==============================] - 34s 15ms/step - d_loss: -0.1171 - g_loss: 26.1565\n","Epoch 2/100\n","2188/2188 [==============================] - 34s 15ms/step - d_loss: 0.2870 - g_loss: 3.6749\n","Epoch 3/100\n","2188/2188 [==============================] - 34s 16ms/step - d_loss: 0.4374 - g_loss: 2.3194\n","Epoch 4/100\n","2188/2188 [==============================] - 34s 16ms/step - d_loss: 0.6198 - g_loss: 1.2425\n","Epoch 5/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6049 - g_loss: 1.1443\n","Epoch 6/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.5925 - g_loss: 1.1332\n","Epoch 7/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.5723 - g_loss: 1.1529\n","Epoch 8/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.5576 - g_loss: 1.1578\n","Epoch 9/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.5504 - g_loss: 1.1737\n","Epoch 10/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.5509 - g_loss: 1.1658\n","Epoch 11/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.5504 - g_loss: 1.1633\n","Epoch 12/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.5569 - g_loss: 1.1117\n","Epoch 13/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.5768 - g_loss: 1.0564\n","Epoch 14/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.5853 - g_loss: 1.0387\n","Epoch 15/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.5904 - g_loss: 1.0249\n","Epoch 16/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.5952 - g_loss: 1.0204\n","Epoch 17/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.5984 - g_loss: 1.0140\n","Epoch 18/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.5997 - g_loss: 1.0116\n","Epoch 19/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6015 - g_loss: 1.0098\n","Epoch 20/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6017 - g_loss: 1.0159\n","Epoch 21/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6016 - g_loss: 1.0152\n","Epoch 22/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6012 - g_loss: 1.0176\n","Epoch 23/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6011 - g_loss: 1.0144\n","Epoch 24/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6023 - g_loss: 1.0141\n","Epoch 25/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6024 - g_loss: 1.0144\n","Epoch 26/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6007 - g_loss: 1.0177\n","Epoch 27/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6017 - g_loss: 1.0202\n","Epoch 28/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6014 - g_loss: 1.0166\n","Epoch 29/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6032 - g_loss: 1.0157\n","Epoch 30/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6016 - g_loss: 1.0137\n","Epoch 31/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6043 - g_loss: 1.0147\n","Epoch 32/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6030 - g_loss: 1.0170\n","Epoch 33/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6051 - g_loss: 1.0121\n","Epoch 34/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6064 - g_loss: 1.0141\n","Epoch 35/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6066 - g_loss: 1.0107\n","Epoch 36/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6100 - g_loss: 1.0126\n","Epoch 37/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6106 - g_loss: 1.0118\n","Epoch 38/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6100 - g_loss: 1.0044\n","Epoch 39/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6114 - g_loss: 1.0099\n","Epoch 40/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6124 - g_loss: 1.0067\n","Epoch 41/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6128 - g_loss: 1.0040\n","Epoch 42/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6117 - g_loss: 0.9986\n","Epoch 43/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6125 - g_loss: 1.0007\n","Epoch 44/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6125 - g_loss: 1.0010\n","Epoch 45/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6119 - g_loss: 1.0070\n","Epoch 46/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6119 - g_loss: 1.0029\n","Epoch 47/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6124 - g_loss: 1.0072\n","Epoch 48/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6130 - g_loss: 1.0010\n","Epoch 49/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6109 - g_loss: 1.0041\n","Epoch 50/100\n","2188/2188 [==============================] - 36s 16ms/step - d_loss: 0.6114 - g_loss: 1.0049\n","Epoch 51/100\n","2188/2188 [==============================] - 36s 16ms/step - d_loss: 0.6097 - g_loss: 1.0048\n","Epoch 52/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6091 - g_loss: 1.0094\n","Epoch 53/100\n","2188/2188 [==============================] - 35s 16ms/step - d_loss: 0.6116 - g_loss: 1.0052\n","Epoch 54/100\n","1094/2188 [==============>...............] - ETA: 17s - d_loss: 0.6121 - g_loss: 1.0016"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bMahI3H3Wbcq"},"source":["Display the last generated images:"]},{"cell_type":"code","metadata":{"id":"-3QcIxqjWbcq"},"source":["from IPython.display import Image, display\n","for i in range(epochs):\n","  display(Image(f\"generated_img_2_{i}.png\"))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FnyYvhsgXT5z"},"source":["display(Image(f\"generated_img_0_{epochs-1}.png\"))\n","display(Image(f\"generated_img_1_{epochs-1}.png\"))\n","display(Image(f\"generated_img_2_{epochs-1}.png\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L7gHAwHCjaAm"},"source":[""],"execution_count":null,"outputs":[]}]}