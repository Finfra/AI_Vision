{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)  # download MNIST data if not exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set  : 55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set      : 10000 samples\n"
     ]
    }
   ],
   "source": [
    "X_train = mnist.train.images\n",
    "Y_train = mnist.train.labels\n",
    "X_validation = mnist.validation.images\n",
    "Y_validation = mnist.validation.labels\n",
    "X_test = mnist.test.images\n",
    "Y_test = mnist.test.labels\n",
    "\n",
    "train_data_size = mnist.train.num_examples # len(X_train)\n",
    "print(\"Training Set  : {0} samples\".format(mnist.train.num_examples))\n",
    "print(\"Validation Set: {0} samples\".format(mnist.validation.num_examples))\n",
    "print(\"Test Set      : {0} samples\".format(mnist.test.num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Image Shape   : (784,)\n",
      "\n",
      "Reshaped Image Shape   : (28, 28, 1)\n",
      "Original digit data: 28 x 28   ->   MNIST image size\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"MNIST Image Shape   : {0}\".format(X_train[0].shape))\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "X_validation = X_validation.reshape(X_validation.shape[0], 28, 28, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')\n",
    "\n",
    "print(\"\\nReshaped Image Shape   : {0}\".format(X_train[0].shape))\n",
    "\n",
    "sample_idx = 139  # random.randint(0, mnist.train.num_examples - 1)\n",
    "\n",
    "print(\"Original digit data: 28 x 28   ->   MNIST image size\")\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(\n",
    "     mnist.train.images[sample_idx:sample_idx + 1].reshape(28, 28),\n",
    "     cmap='Greys',\n",
    "     interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expand Image Size: (32, 32, 1) <- LeNet's Input Size is 32 x 32\n",
      "\n",
      "Resized digit data: 32 x 32   ->   LeNet input size\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 86.4x86.4 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y (label) data, one-hot style\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# zero padding\n",
    "# pad(data, ((up, down), (left, right)), 'constant')\n",
    "X_train = np.pad(X_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "X_validation = np.pad(X_validation, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "X_test = np.pad(X_test, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "\n",
    "print(\"Expand Image Size: {} <- LeNet's Input Size is 32 x 32\".format(X_train[0].shape))\n",
    "\n",
    "print(\"\\nResized digit data: 32 x 32   ->   LeNet input size\")\n",
    "plt.figure(figsize=(1.2,1.2))\n",
    "image = X_train[sample_idx:sample_idx + 1].squeeze()\n",
    "plt.imshow(\n",
    "     image,\n",
    "     cmap='Greys',\n",
    "     interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "print(\"Y (label) data, one-hot style\")\n",
    "print(Y_train[sample_idx:sample_idx + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "epoch_size    = 10    # tr data 10번 반복해서 학습\n",
    "batch_length  = 100   # 1 epoch (tr data 한번 학습) 시 100개씩 배치로 읽어\n",
    "train_batch_size      = int(mnist.train.num_examples / batch_length)       # 55000 / 100 = 550\n",
    "validation_batch_size = int(mnist.validation.num_examples / batch_length)  # 5000  / 100 =  50\n",
    "test_batch_size       = int(mnist.test.num_examples / batch_length)        # 10000 / 100 = 100\n",
    "\n",
    "input_width   = 32    # resized MNIST data image 32 x 32\n",
    "input_height  = 32    # resized MNIST data image 32 x 32\n",
    "input_channel = 1     # MNIST image color: black or white\n",
    "output_size   = 10    # 0 ~ 9 까지 10개 클래스를 one-hot encoding 값으로 사용할 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "# Build LeNet-5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(feature_map, filter_size, input_channel, output_channel, stride, padding='VALID', mean=0, stddev=0.1):     \n",
    "    weight_shape = [filter_size, filter_size, input_channel, output_channel]\n",
    "    conv_kernel = tf.Variable(tf.truncated_normal(shape=weight_shape, mean=mean, stddev=stddev))\n",
    "    \n",
    "    bias_shape = [1, stride, stride,1]\n",
    "    conv_bias = tf.Variable(tf.random_normal([output_channel]))\n",
    "    \n",
    "    conv = tf.nn.conv2d(feature_map, conv_kernel, strides=bias_shape, padding=padding) + conv_bias  \n",
    "    conv = tf.nn.relu(conv)\n",
    "    return conv\n",
    "\n",
    "def pool_layer(feature_map, kernel_size, stride, padding='VALID'):     \n",
    "    pool = tf.nn.max_pool(feature_map, \n",
    "                          ksize  =[1, kernel_size, kernel_size,1], \n",
    "                          strides=[1, stride, stride,1], \n",
    "                          padding=padding)\n",
    "    return pool\n",
    "\n",
    "def fc_layer(input_data, input_size, output_size, mean=0, stddev=0.1):     \n",
    "    fc_weight = tf.Variable(tf.truncated_normal(shape=[input_size, output_size], mean=mean, stddev=stddev))\n",
    "    fc_bias = tf.Variable(tf.random_normal([output_size]))\n",
    "    fc = tf.matmul(input_data, fc_weight) + fc_bias\n",
    "    fc = tf.nn.relu(fc)  # activation\n",
    "    return fc\n",
    "\n",
    "def fc_softmax_layer(input_data, input_size, output_size, mean=0, stddev=0.1):     \n",
    "    fc_weight = tf.Variable(tf.truncated_normal(shape=[input_size, output_size], mean=mean, stddev=stddev))\n",
    "    fc_bias = tf.Variable(tf.random_normal([output_size]))\n",
    "    logits = tf.matmul(input_data, fc_weight) + fc_bias\n",
    "    h = tf.nn.softmax(logits)  # Hypothesis (using softmax)\n",
    "    return logits, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('input') as scope:\n",
    "    X = tf.placeholder(tf.float32, [None, input_width, input_height, input_channel], name='x_input')\n",
    "    Y = tf.placeholder(tf.float32, [None, output_size], name='y_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_layer1 (32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# Input = 32x32x1. Output = 28x28x6.\n",
    "with tf.name_scope('conv_layer1') as scope:\n",
    "    print('conv_layer1', X[0].shape)\n",
    "    conv1 = conv_layer(X, filter_size=5, input_channel=1, output_channel=6, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_layer1 (28, 28, 6)\n"
     ]
    }
   ],
   "source": [
    "# Input = 28x28x6. Output = 14x14x6.\n",
    "with tf.name_scope('pooling_layer1') as scope:\n",
    "    print('conv_layer1', conv1[0].shape)\n",
    "    pool1 = pool_layer(conv1, kernel_size=2, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input = 14x14x6.. Output = 10x10x16\n",
    "with tf.name_scope('conv_layer2') as scope:\n",
    "    conv2 = conv_layer(pool1, filter_size=5, input_channel=6, output_channel=16, stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input = 10x10x16. Output = 5x5x16.\n",
    "with tf.name_scope('pooling_layer2') as scope:\n",
    "    pool2 = pool_layer(conv2, kernel_size=2, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input = 5x5x16. Output = 400x1.\n",
    "with tf.name_scope('flatten_layer') as scope:\n",
    "    flattened = tf.contrib.layers.flatten(pool2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input = 400x1. Output = 120x1.\n",
    "with tf.name_scope('fc_layer1') as scope:\n",
    "    fc1 = fc_layer(flattened, input_size=400, output_size=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input = 120x1. Output = 84x1.\n",
    "with tf.name_scope('fc_layer2') as scope:\n",
    "    fc2 = fc_layer(fc1, input_size=120, output_size=84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input = 84x1. Output = 10x1.\n",
    "with tf.name_scope('fc_layer3') as scope:\n",
    "    logits, h = fc_softmax_layer(fc2, input_size=84, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"cost\") as scope:\n",
    "    # cost function: cross entropy cost/loss\n",
    "    # cost function    L = 1/N * Σ D(S(WXi + b), Li)    cost function L은 loss 의미 \n",
    "    # cross entropy    D(S, L) = - Σ Li log(Si)         D(S, Li)의 L은 label 의미\n",
    "    # cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(h), axis = 1))\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y)\n",
    "    cost = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(cost)\n",
    "    \n",
    "    cost_summ = tf.summary.scalar('cost', cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy') as scope:\n",
    "    \n",
    "    with tf.name_scope('correct_prediction') as scope:\n",
    "        correct_prediction = tf.equal(tf.argmax(h, 1), tf.argmax(Y, 1))\n",
    "    with tf.name_scope('correct') as scope:\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "    accuracy_summ = tf.summary.scalar(\"accuracy\", accuracy, ['merged_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "# Run Train & Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions to help batch training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_train(data, batch_size, merged_summary, writer, epoch):     \n",
    "    sum_cost = 0\n",
    "    for step in range(batch_size):\n",
    "        batch_x, batch_y = data.next_batch(batch_length)\n",
    "        batch_x = batch_x.reshape(batch_x.shape[0], 28, 28, 1).astype('float32')\n",
    "        batch_x = np.pad(batch_x, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "                \n",
    "        summary, cost_res, _ = sess.run([merged_summary, cost, train],\n",
    "                                      feed_dict = {X: batch_x,\n",
    "                                                   Y: batch_y})\n",
    "        \n",
    "        #writer.add_summary(summary, epoch * batch_size + step)\n",
    "        sum_cost += cost_res\n",
    "\n",
    "    return sum_cost / batch_size\n",
    "\n",
    "def process_validation(data, batch_size, merged_validation_accuracy, writer, epoch):\n",
    "    sum_accuracy = 0\n",
    "    for step in range(batch_size):\n",
    "        batch_x, batch_y = data.next_batch(batch_length)\n",
    "        batch_x = batch_x.reshape(batch_x.shape[0], 28, 28, 1).astype('float32')\n",
    "        batch_x = np.pad(batch_x, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "\n",
    "        _accuracy = sess.run([accuracy], feed_dict = {X: batch_x, Y: batch_y})\n",
    "        sum_accuracy += _accuracy[0] * batch_length\n",
    "        \n",
    "        merged = sess.run([merged_validation_accuracy],\n",
    "                feed_dict = {X: batch_x, Y: batch_y})\n",
    "        \n",
    "        #writer.add_summary(merged[0], epoch * batch_size + step)\n",
    "\n",
    "    return sum_accuracy / batch_size\n",
    "\n",
    "def process_test(data, batch_size):   \n",
    "    sum_accuracy = 0\n",
    "    for step in range(batch_size):\n",
    "        batch_x, batch_y = data.next_batch(batch_length)\n",
    "        batch_x = batch_x.reshape(batch_x.shape[0], 28, 28, 1).astype('float32')\n",
    "        batch_x = np.pad(batch_x, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "\n",
    "        _accuracy = sess.run([accuracy], feed_dict = {X: batch_x, Y: batch_y})\n",
    "        sum_accuracy += _accuracy[0] * batch_length\n",
    "\n",
    "    return sum_accuracy / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:\n",
      "\n",
      "[epoch 01]                cost : 00.481792852 \n",
      "           validation accuracy : 96.320000529 \n",
      "\n",
      "[epoch 02]                cost : 00.101589218 \n",
      "           validation accuracy : 97.600000739 \n",
      "\n",
      "[epoch 03]                cost : 00.070699629 \n",
      "           validation accuracy : 97.940001130 \n",
      "\n",
      "[epoch 04]                cost : 00.054719911 \n",
      "           validation accuracy : 98.260001063 \n",
      "\n",
      "[epoch 05]                cost : 00.045999550 \n",
      "           validation accuracy : 98.340000629 \n",
      "\n",
      "[epoch 06]                cost : 00.038016677 \n",
      "           validation accuracy : 98.520001292 \n",
      "\n",
      "[epoch 07]                cost : 00.032195558 \n",
      "           validation accuracy : 98.500001192 \n",
      "\n",
      "[epoch 08]                cost : 00.028294906 \n",
      "           validation accuracy : 98.680001020 \n",
      "\n",
      "[epoch 09]                cost : 00.024726508 \n",
      "           validation accuracy : 98.760001063 \n",
      "\n",
      "[epoch 10]                cost : 00.020866316 \n",
      "           validation accuracy : 98.680001140 \n",
      "\n",
      "\n",
      "Test accuracy:  98.79000091552734\n",
      "\n",
      "\n",
      "Visualizing a single test\n",
      "answer    :  [5]\n",
      "prediction:  [5]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 158.4x158.4 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    with tf.name_scope(\"train\") as scope:     \n",
    "        # tensorboard\n",
    "        merged_summary = tf.summary.merge_all()\n",
    "        merged_validation_accuracy = tf.summary.merge_all('merged_accuracy')\n",
    "        #writer = tf.summary.FileWriter('logs/lenet_5_logs')\n",
    "        #writer.add_graph(sess.graph)\n",
    "        writer = []\n",
    "        \n",
    "        # initialize tf variables\n",
    "        sess.run(tf.global_variables_initializer())  \n",
    "        \n",
    "        # training cycle\n",
    "        print(\"\\nTraining:\\n\")\n",
    "        for epoch in range(epoch_size):\n",
    "            \n",
    "            # a) current epoch's learning\n",
    "            avg_cost = process_train(\n",
    "                mnist.train, train_batch_size,\n",
    "                merged_summary, writer, epoch)\n",
    "            \n",
    "            # b) validation accuracy\n",
    "            validation_accuracy = process_validation(\n",
    "                mnist.validation, validation_batch_size,\n",
    "                merged_validation_accuracy, writer, epoch)\n",
    "\n",
    "            print('[epoch {:02d}]'.format(epoch + 1),\n",
    "                  '               cost : {:012.9f}'.format(avg_cost), '\\n          ',\n",
    "                  'validation accuracy : {:012.9f}'.format(validation_accuracy) , '\\n')\n",
    "            \n",
    "    with tf.name_scope(\"test\") as scope:\n",
    "        test_accuracy = process_test(mnist.test, test_batch_size)\n",
    "        print(\"\\nTest accuracy: \", test_accuracy)\n",
    "            \n",
    "        # Vsualization of a single test\n",
    "        # randomly get a number and it's predict\n",
    "        print(\"\\n\\nVisualizing a single test\")\n",
    "        r = random.randint(0, mnist.test.num_examples - 1)\n",
    "        print(\"answer    : \", sess.run(tf.argmax(Y_test[r:r + 1], 1)))\n",
    "        print(\"prediction: \", sess.run(tf.argmax(h, 1),\n",
    "                                       feed_dict={X: X_test[r:r + 1]}))\n",
    "        \n",
    "        plt.figure(figsize=(2.2,2.2))\n",
    "        image = X_test[r:r + 1].squeeze()\n",
    "        plt.imshow(\n",
    "             image,\n",
    "             cmap='Greys',\n",
    "             interpolation='nearest')\n",
    "        plt.show()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
